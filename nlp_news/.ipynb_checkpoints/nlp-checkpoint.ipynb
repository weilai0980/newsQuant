{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *\n",
    "import urllib\n",
    "\n",
    "from __future__ import division  # Python 2 users only\n",
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import anago\n",
    "from anago.utils import download\n",
    "\n",
    "import metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pympler import asizeof\n",
    "asizeof.asizeof([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{'cryptocurrency': {0, 1, 5, 12, 13, 14, 15}} \n",
    "\n",
    " {\".BEST's\": {12, 6}, 'Nation\"': {13}, 'BestCoin': {13}, 'ICANN': {8},\n",
    "  'Numbers),': {8}, 'Corporation': {8}, 'Cyril Fremont': {12, 6, 7}, 'French': {1, 2, 3, 12, 9}, \n",
    "  'Social': {15}, 'Tim Sandle': {1}, 'Pty Ltd': {8, 2, 10, 7}, 'Pre-registrations PeopleBrowsr': {6}, \n",
    "  'San Francisco': {6}, 'Business Paris': {1}, 'Internet.BestTLD Pty Ltd': {8}, 'Paris': {7}, \n",
    "  'Best SAS': {2, 7}, 'Crypto.\"': {14}, 'Internet': {9}, 'Top Level Domain': {6}, 'GoDaddy,': {9}, \n",
    "  'Crypto-Currency': {13}, 'Assigned Names': {8}, 'American': {9}, \n",
    "  'The Best SAS,': {7}, 'Digital Territory': {13}, 'TLD': {8, 10, 3, 13, 7}, \n",
    "  'Fremont:': {14}, 'BestTLD Pty Ltd': {8, 10, 7}, 'So,': {13}, 'Social network, Rewards,': {15}, \n",
    "  'Cyril Fremont.': {7}, 'TLD),': {13}, 'Generic Top Level Domain BestTLD Pty Ltd': {2}, 'Best Nation': {14}} \n",
    "\n",
    " {('PER', 'So,'), ('LOC', 'San Francisco'), ('MISC', 'American'), ('MISC', 'Pre-registrations PeopleBrowsr'), \n",
    "  ('2150302', 'Pty Ltd'), ('ORG', 'Top Level Domain'), ('ORG', 'Digital Territory'), ('MISC', 'GoDaddy,'), \n",
    "  ('MISC', 'Crypto-Currency'), ('PER', 'Cyril Fremont.'), ('MISC', \".BEST's\"), ('ORG', 'BestTLD Pty Ltd'),\n",
    "  ('ORG', 'Best SAS'), ('ORG', 'Generic Top Level Domain BestTLD Pty Ltd'), ('MISC', 'Social network, Rewards,'),\n",
    "  ('ORG', 'TLD'), ('PER', 'TLD),'), ('LOC', 'Paris'), ('LOC', 'Business Paris'), ('ORG', 'Best Nation'),\n",
    "  ('ORG', 'ICANN'), ('MISC', 'BestCoin'), ('MISC', 'Assigned Names'), ('ORG', 'The Best SAS,'), \n",
    "  ('MISC', 'Fremont:'), ('MISC', 'Crypto.\"'), ('PER', 'Tim Sandle'), ('ORG', 'Internet.BestTLD Pty Ltd'),\n",
    "  ('MISC', 'Social'), ('PER', 'Cyril Fremont'), ('ORG', 'Numbers),'), ('ORG', \".BEST's\"),\n",
    "  ('ORG', 'Corporation'), ('MISC', 'Internet'), ('PER', 'Nation\"'), ('MISC', 'French')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decaNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Check whether you have started the CoreNLP server e.g.\n$ cd stanford-corenlp-full-2015-12-09/ \n$ java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 171\u001b[0;31m                 (self._dns_host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m   1105\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m   1150\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body)\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage_body\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    878\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    179\u001b[0m             raise NewConnectionError(\n\u001b[0;32m--> 180\u001b[0;31m                 self, \"Failed to establish a new connection: %s\" % e)\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7fadbf166a90>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    444\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m                 )\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    637\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[0;32m--> 638\u001b[0;31m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[1;32m    639\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=9000): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fadbf166a90>: Failed to establish a new connection: [Errno 111] Connection refused',))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pycorenlp/corenlp.py\u001b[0m in \u001b[0;36mannotate\u001b[0;34m(self, text, properties)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=9000): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fadbf166a90>: Failed to establish a new connection: [Errno 111] Connection refused',))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e14a7b5f3320>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i am very happy today!'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pycorenlp/corenlp.py\u001b[0m in \u001b[0;36mannotate\u001b[0;34m(self, text, properties)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             raise Exception('Check whether you have started the CoreNLP server e.g.\\n'\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;34m'$ cd stanford-corenlp-full-2015-12-09/ \\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             '$ java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer')\n",
      "\u001b[0;31mException\u001b[0m: Check whether you have started the CoreNLP server e.g.\n$ cd stanford-corenlp-full-2015-12-09/ \n$ java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer"
     ]
    }
   ],
   "source": [
    "nlp.annotate(\"i am very happy today!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['BTC', 'Bitcoin'], ['ETH', 'Ethereum'], ['XRP', 'Ripple'], ['BCH', 'Bitcoin Cash'], ['EOS', 'EOS'], ['LTC', 'Litecoin'], ['ADA', 'Cardano'], ['XLM', 'Stellar'], ['MIOTA', 'IOTA'], ['TRX', 'TRON'], ['NEO', 'NEO'], ['XMR', 'Monero'], ['DASH', 'Dash'], ['USDT', 'Tether'], ['XEM', 'NEM'], ['VEN', 'VeChain'], ['BNB', 'Binance Coin'], ['ETC', 'Ethereum Classic'], ['QTUM', 'Qtum'], ['ONT', 'Ontology'], ['OMG', 'OmiseGO'], ['BCN', 'Bytecoin'], ['ICX', 'ICON'], ['LSK', 'Lisk'], ['ZEC', 'Zcash'], ['ZIL', 'Zilliqa'], ['BTG', 'Bitcoin Gold'], ['AE', 'Aeternity'], ['DCR', 'Decred'], ['ZRX', '0x'], ['BTM', 'Bytom'], ['STEEM', 'Steem'], ['XVG', 'Verge'], ['BTS', 'BitShares'], ['RHOC', 'RChain'], ['SC', 'Siacoin'], ['NANO', 'Nano'], ['MKR', 'Maker'], ['GNT', 'Golem'], ['WAN', 'Wanchain'], ['PPT', 'Populous'], ['BCD', 'Bitcoin Diamond'], ['STRAT', 'Stratis'], ['WAVES', 'Waves'], ['BTCP', 'Bitcoin Private'], ['DOGE', 'Dogecoin'], ['REP', 'Augur'], ['IOST', 'IOST'], ['WTC', 'Waltonchain'], ['DGB', 'DigiByte'], ['SNT', 'Status'], ['NAS', 'Nebulas'], ['HSR', 'Hshare'], ['WICC', 'WaykiChain'], ['XIN', 'Mixin'], ['AION', 'Aion'], ['DGD', 'DigixDAO'], ['HT', 'Huobi Token'], ['LRC', 'Loopring'], ['BAT', 'Basic Attention Token'], ['KMD', 'Komodo'], ['ELF', 'aelf'], ['ARK', 'Ark'], ['ARDR', 'Ardor'], ['CMT', 'CyberMiles'], ['GXS', 'GXChain'], ['PIVX', 'PIVX'], ['MAID', 'MaidSafeCoin'], ['BNT', 'Bancor'], ['DCN', 'Dentacoin'], ['LOOM', 'Loom Network'], ['MOAC', 'MOAC'], ['ELA', 'Elastos'], ['CNX', 'Cryptonex'], ['MONA', 'MonaCoin'], ['SKY', 'Skycoin'], ['GAS', 'Gas'], ['KNC', 'Kyber Network'], ['CTXC', 'Cortex'], ['MITH', 'Mithril'], ['POLY', 'Polymath'], ['SYS', 'Syscoin'], ['RDD', 'ReddCoin'], ['QASH', 'QASH'], ['VERI', 'Veritaseum'], ['ETHOS', 'Ethos'], ['FSN', 'Fusion'], ['FUN', 'FunFair'], ['THETA', 'Theta Token'], ['CENNZ', 'Centrality'], ['NULS', 'Nuls'], ['SUB', 'Substratum'], ['NXS', 'Nexus'], ['ENG', 'Enigma'], ['DRGN', 'Dragonchain'], ['KIN', 'Kin'], ['NXT', 'Nxt'], ['ETN', 'Electroneum'], ['BIX', 'Bibox Token'], ['SOC', 'All Sports']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "crypto_entity = []\n",
    "\n",
    "para_path = './metadata/top_cc.txt'\n",
    "\n",
    "with open(para_path, \"r\") as ins:\n",
    "    array = []\n",
    "    para_dict = {}\n",
    "    \n",
    "    for line in ins:\n",
    "        newline = line.strip('\\n')\n",
    "        tmpline = newline.split(',')\n",
    "        \n",
    "        crypto_entity.append( [tmpline[0], tmpline[1].lstrip(' ')] )\n",
    "        \n",
    "\n",
    "print(crypto_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "lower case\n",
    "stop word\n",
    "stem of words\n",
    "punctuation \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "http://web.stanford.edu/class/cs224n/syllabus.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n---- basic concepts:\\n\\nHypernyms \\n\\nhyponyms\\n\\nmeronyms\\n\\nholonyms\\n\\nFor example, the parts of a tree are its trunk, crown, and so on; the part_meronyms(). \\nThe substance a tree is made of includes heartwood and sapwood; the substance_meronyms(). \\nA collection of trees forms a forest; the member_holonyms():\\n\\nanaphora resolution.\\n\\ncoreference \\n\\n---- Manual features:\\n \\n document/sentence - term features\\n punct_features for sentence segmentation \\n \\n bag of words\\n \\n \\n---- Dataset:\\n\\n Brow: Part of Speech Tagged\\n Moive review: sentiment\\n TreeBank:\\n nps_chat: dialogue acts\\n \\n Penn Treebank: POS tag set, NER\\n \\n CoNLL 2000 Corpus: POS tags, IOB chunk tags\\n \\n IMDB reviews: sentiment \\n\\n\\n---- Useful tools in NLTK\\n\\nfdist1 = FreqDist(['32','12','est','est'])\\nprint fdist1.items()\\n\\nnames = nltk.corpus.names\\n\\nfrom nltk.corpus import stopwords\\nstopwords.words('english')\\n\\nfrom nltk.corpus import sw1adesh\\n 200 common words in several languages.\\n \\nfrom nltk.corpus import wordnet as wn\\n\\n--- pre-processing\\n\\nNER:\\n\\nNo low-case in NER \\n\\n\\nEmbedding:\\n\\n similarity comparison: \\n    \\n    low-case\\n    remove single quote character (‘)\\n    \\n classification: NO low case\\n \\n \\n acronym: keep\\n\\nClassification:\\n\\n--- embedding\\n\\nFine-tuning:  \\n  word vectors should not be retrained if the training data set is small. \\n  If the training set is large, retraining may improve performance\\n\\nEvaluation:\\n\\n Intrinsic:\\n  Word Vector Analogies\\n  Correlation Evaluation\\n  \\n Extrinsic:\\n  classification\\n  \\n  \\n  http://cs224d.stanford.edu/lecture_notes/notes2.pdf\\n\\n--- language model\\n\\nEvaluation:\\n  \\n  application specific metric\\n  \\n  perplexity\\n  \\n  \\n\\n\\nout of vocabulary (OOV), <UNK>: replace by <UNK> all words that occur fewer than n times in the training set, \\n\\n\\n  \\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "---- basic concepts:\n",
    "\n",
    "Hypernyms \n",
    "\n",
    "hyponyms\n",
    "\n",
    "meronyms\n",
    "\n",
    "holonyms\n",
    "\n",
    "For example, the parts of a tree are its trunk, crown, and so on; the part_meronyms(). \n",
    "The substance a tree is made of includes heartwood and sapwood; the substance_meronyms(). \n",
    "A collection of trees forms a forest; the member_holonyms():\n",
    "\n",
    "anaphora resolution.\n",
    "\n",
    "coreference \n",
    "\n",
    "---- Manual features:\n",
    " \n",
    " document/sentence - term features\n",
    " punct_features for sentence segmentation \n",
    " \n",
    " bag of words\n",
    " \n",
    " \n",
    "---- Dataset:\n",
    "\n",
    " Brow: Part of Speech Tagged\n",
    " Moive review: sentiment\n",
    " TreeBank:\n",
    " nps_chat: dialogue acts\n",
    " \n",
    " Penn Treebank: POS tag set, NER\n",
    " \n",
    " CoNLL 2000 Corpus: POS tags, IOB chunk tags\n",
    " \n",
    " IMDB reviews: sentiment \n",
    "\n",
    "\n",
    "---- Useful tools in NLTK\n",
    "\n",
    "fdist1 = FreqDist(['32','12','est','est'])\n",
    "print fdist1.items()\n",
    "\n",
    "names = nltk.corpus.names\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')\n",
    "\n",
    "from nltk.corpus import sw1adesh\n",
    " 200 common words in several languages.\n",
    " \n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "--- pre-processing\n",
    "\n",
    "NER:\n",
    "\n",
    "  No low-case in NER \n",
    "\n",
    "Embedding:\n",
    "\n",
    "  similarity comparison: \n",
    "    \n",
    "    low-case\n",
    "    remove single quote character (‘)\n",
    "    \n",
    "  classification: NO low case\n",
    " \n",
    "  acronym: keep\n",
    "\n",
    "Classification:\n",
    "\n",
    "\n",
    "--- entity \n",
    "\n",
    "Given a knowledge base (KB), usually in the form of a\n",
    "graph, and a document d with mentions marked up (usually\n",
    "through a named entity recognition process), entity linking\n",
    "aims at assigning unique entities from the KB to those mentions,\n",
    "whenever appropriate. More precisely:\n",
    "\n",
    "\n",
    "entity resolution\n",
    "\n",
    "coreference\n",
    "\n",
    "linking\n",
    "\n",
    "\n",
    "--- embedding\n",
    "\n",
    "Fine-tuning:  \n",
    "\n",
    "  word vectors should not be retrained if the training data set is small. \n",
    "  If the training set is large, retraining may improve performance\n",
    "\n",
    "Evaluation:\n",
    "\n",
    "  Intrinsic:\n",
    "    Word Vector Analogies\n",
    "    Correlation Evaluation\n",
    "  \n",
    "  Extrinsic:\n",
    "    classification\n",
    "  \n",
    "  http://cs224d.stanford.edu/lecture_notes/notes2.pdf\n",
    "\n",
    "--- language model\n",
    "\n",
    "Evaluation:\n",
    "  \n",
    "  application specific metric\n",
    "  \n",
    "  perplexity\n",
    "  \n",
    " \n",
    "Pre-processing:\n",
    "\n",
    "  out of vocabulary (OOV), <UNK>: replace by <UNK> all words that occur fewer than n times in the training set, \n",
    "  \n",
    "  Smoothing:\n",
    "    add one or k smoothing\n",
    "    \n",
    "    *interpolated Kneser-Ney algorithm\n",
    "\n",
    "  \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\r\n",
      "1176965\n"
     ]
    }
   ],
   "source": [
    "# text from Websites\n",
    "import urllib;\n",
    "\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = urllib.urlopen(url)\n",
    "tmp_raw = response.read()\n",
    "\n",
    "# tmp = tmp_raw.decode('utf8')\n",
    "# raw = tmp_raw.encode('utf8')\n",
    "# print type(tmp), len(tmp)\n",
    "\n",
    "raw = tmp_raw.decode('utf8')\n",
    "print raw[:75]\n",
    "print len(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: <type 'list'> 257726\n",
      "text:  <class 'nltk.text.Text'> [u'\\ufeffThe', u'Project', u'Gutenberg', u'EBook', u'of', u'Crime', u'and', u'Punishment', u',', u'by']\n",
      "collocations: \n",
      "Katerina Ivanovna; Pyotr Petrovitch; Pulcheria Alexandrovna; Avdotya\n",
      "Romanovna; Rodion Romanovitch; Marfa Petrovna; Sofya Semyonovna; old\n",
      "woman; Project Gutenberg-tm; Porfiry Petrovitch; Amalia Ivanovna;\n",
      "great deal; young man; Nikodim Fomitch; Ilya Petrovitch; Project\n",
      "Gutenberg; Andrey Semyonovitch; Hay Market; Dmitri Prokofitch; Good\n",
      "heavens\n",
      "None\n",
      "words: <type 'list'> <type 'unicode'> 257726\n",
      "vocabulary: <type 'list'> <type 'unicode'> 10705\n",
      "<type 'unicode'>\n",
      "5336\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "# Dealing with raw text\n",
    "\n",
    "# Tokenize \n",
    "tokens = word_tokenize(raw)\n",
    "print 'tokens:', type(tokens), len(tokens)\n",
    "\n",
    "# Stemmers\n",
    "def stem(word):\n",
    "    \n",
    "    for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    \n",
    "    return word\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "# Lemmatization\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "# [wnl.lemmatize(t) for t in tokens]\n",
    "\n",
    "token_stem = [porter.stem(t) for t in tokens]\n",
    "\n",
    "# Build text \n",
    "text = nltk.Text(tokens)\n",
    "print 'text: ', type(text), text[:10]\n",
    "\n",
    "# Collocation \n",
    "print 'collocations: \\n', text.collocations()\n",
    "\n",
    "# lower case\n",
    "words = [w.lower() for w in tokens]\n",
    "print 'words:', type(words), type(words[0]), len(words)\n",
    "\n",
    "# vocabulary \n",
    "vocab = sorted(set(words))\n",
    "print 'vocabulary:', type(vocab), type(vocab[0]), len(vocab)\n",
    "\n",
    "\n",
    "print type(raw)\n",
    "print raw.find(\"PART I\")\n",
    "# 5338\n",
    "print raw.rfind(\"End of Project Gutenberg's Crime\")\n",
    "# 1157743\n",
    "# raw = raw[5338:1157743] [1]\n",
    "# raw.find(\"PART I\")\n",
    "\n",
    "\n",
    "# only alphabetical \n",
    "words=[ word.lower() for word in words if word.isalpha() ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'unicode'>\n",
      "sentence:  <type 'list'> [u'[The Man Who Was Thursday by G. K. Chesterton 1908]\\n\\nTo Edmund Clerihew Bentley\\n\\nA cloud was on the mind of men, and wailing went the weather,\\nYea, a sick cloud upon the soul when we were boys together.', u'Science announced nonentity and art admired decay;\\nThe world was old and ended: but you and I were gay;\\nRound us in antic order their crippled vices came--\\nLust that had lost its laughter, fear that had lost its shame.']\n"
     ]
    }
   ],
   "source": [
    "text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')\n",
    "print type(text)\n",
    "\n",
    "sents = nltk.sent_tokenize(text)\n",
    "\n",
    "print 'sentence: ', type(sents), sents[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "brown_sents = brown.sents(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd = nltk.FreqDist(brown.words(categories='news'))\n",
    "cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news'))\n",
    "most_freq_words = fd.most_common(100)\n",
    "likely_tags = dict((word, cfd[word].max()) for (word, _) in most_freq_words)\n",
    "baseline_tagger = nltk.UnigramTagger(model=likely_tags)\n",
    "baseline_tagger.evaluate(brown_tagged_sents)\n",
    "# 0.45578495136941344\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Automatic Tagging\n",
    "\n",
    "from nltk.corpus import brown\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "brown_sents = brown.sents(categories='news')\n",
    "unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\n",
    "unigram_tagger.tag(brown_sents[2007])\n",
    "# [('Various', 'JJ'), ('of', 'IN'), ('the', 'AT'), ('apartments', 'NNS'),\n",
    "# ('are', 'BER'), ('of', 'IN'), ('the', 'AT'), ('terrace', 'NN'), ('type', 'NN'),\n",
    "# (',', ','), ('being', 'BEG'), ('on', 'IN'), ('the', 'AT'), ('ground', 'NN'),\n",
    "# ('floor', 'NN'), ('so', 'QL'), ('that', 'CS'), ('entrance', 'NN'), ('is', 'BEZ'),\n",
    "# ('direct', 'JJ'), ('.', '.')]\n",
    "\n",
    "unigram_tagger.evaluate(brown_tagged_sents)\n",
    "\n",
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "test_sents = brown_tagged_sents[size:]\n",
    "unigram_tagger = nltk.UnigramTagger(train_sents)\n",
    "unigram_tagger.evaluate(test_sents)\n",
    "\n",
    "\n",
    "bigram_tagger = nltk.BigramTagger(train_sents)\n",
    "bigram_tagger.tag(brown_sents[2007])\n",
    "# [('Various', 'JJ'), ('of', 'IN'), ('the', 'AT'), ('apartments', 'NNS'),\n",
    "# ('are', 'BER'), ('of', 'IN'), ('the', 'AT'), ('terrace', 'NN'),\n",
    "# ('type', 'NN'), (',', ','), ('being', 'BEG'), ('on', 'IN'), ('the', 'AT'),\n",
    "# ('ground', 'NN'), ('floor', 'NN'), ('so', 'CS'), ('that', 'CS'),\n",
    "# ('entrance', 'NN'), ('is', 'BEZ'), ('direct', 'JJ'), ('.', '.')]\n",
    "\n",
    "unseen_sent = brown_sents[4203]\n",
    "bigram_tagger.tag(unseen_sent)\n",
    "# [('The', 'AT'), ('population', 'NN'), ('of', 'IN'), ('the', 'AT'), ('Congo', 'NP'),\n",
    "# ('is', 'BEZ'), ('13.5', None), ('million', None), (',', None), ('divided', None),\n",
    "# ('into', None), ('at', None), ('least', None), ('seven', None), ('major', None),\n",
    "# ('``', None), ('culture', None), ('clusters', None), (\"''\", None), ('and', None),\n",
    "# ('innumerable', None), ('tribes', None), ('speaking', None), ('400', None),\n",
    "# ('separate', None), ('dialects', None), ('.', None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From Lists to Strings\n",
    "\n",
    "silly = ['We', 'called', 'him', 'Tortoise', 'because', 'he', 'taught', 'us', '.']\n",
    "' '.join(silly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 5 matches:\n",
      "they say too few people now carry the gene for blondes to last beyond the next t\n",
      " blonde hair is caused by a recessive gene . In order for a child to have blonde\n",
      "o have blonde hair , it must have the gene on both sides of the family in the gr\n",
      "here is a disadvantage of having that gene or by chance . They do n't disappear \n",
      "ndes would disappear is if having the gene was a disadvantage and I do not think\n"
     ]
    }
   ],
   "source": [
    "# Dealing with HTML\n",
    "\n",
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "html = urllib.urlopen(url).read().decode('utf8')\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "raw = BeautifulSoup(html).get_text()\n",
    "tokens = word_tokenize(raw)\n",
    "# print tokens\n",
    "\n",
    "tokens = tokens[110:390]\n",
    "text = nltk.Text(tokens)\n",
    "text.concordance('gene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# local files\n",
    "f = open('document.txt', 'rU')\n",
    "\n",
    "for line in f:\n",
    "    print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'The', u'DET'), (u'Fulton', u'NOUN'), (u'County', u'NOUN'), (u'Grand', u'ADJ'), (u'Jury', u'NOUN'), (u'said', u'VERB'), (u'Friday', u'NOUN'), (u'an', u'DET'), (u'investigation', u'NOUN'), (u'of', u'ADP')]\n"
     ]
    }
   ],
   "source": [
    "# Part of Speech Tagged corpora \n",
    "\n",
    "# ! Brown dataset\n",
    "\n",
    "from nltk.corpus import brown\n",
    "nltk.corpus.brown.tagged_words()\n",
    "\n",
    "brown_news_tagged = brown.tagged_words(categories='news', tagset='universal')\n",
    "tag_fd = nltk.FreqDist(tag for (word, tag) in brown_news_tagged)\n",
    "tag_fd.most_common()\n",
    "\n",
    "print brown_news_tagged[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(u'The', u'DET'),\n",
       "  (u'Fulton', u'NOUN'),\n",
       "  (u'County', u'NOUN'),\n",
       "  (u'Grand', u'ADJ'),\n",
       "  (u'Jury', u'NOUN'),\n",
       "  (u'said', u'VERB'),\n",
       "  (u'Friday', u'NOUN'),\n",
       "  (u'an', u'DET'),\n",
       "  (u'investigation', u'NOUN'),\n",
       "  (u'of', u'ADP'),\n",
       "  (u\"Atlanta's\", u'NOUN'),\n",
       "  (u'recent', u'ADJ'),\n",
       "  (u'primary', u'NOUN'),\n",
       "  (u'election', u'NOUN'),\n",
       "  (u'produced', u'VERB'),\n",
       "  (u'``', u'.'),\n",
       "  (u'no', u'DET'),\n",
       "  (u'evidence', u'NOUN'),\n",
       "  (u\"''\", u'.'),\n",
       "  (u'that', u'ADP'),\n",
       "  (u'any', u'DET'),\n",
       "  (u'irregularities', u'NOUN'),\n",
       "  (u'took', u'VERB'),\n",
       "  (u'place', u'NOUN'),\n",
       "  (u'.', u'.')],\n",
       " [(u'The', u'DET'),\n",
       "  (u'jury', u'NOUN'),\n",
       "  (u'further', u'ADV'),\n",
       "  (u'said', u'VERB'),\n",
       "  (u'in', u'ADP'),\n",
       "  (u'term-end', u'NOUN'),\n",
       "  (u'presentments', u'NOUN'),\n",
       "  (u'that', u'ADP'),\n",
       "  (u'the', u'DET'),\n",
       "  (u'City', u'NOUN'),\n",
       "  (u'Executive', u'ADJ'),\n",
       "  (u'Committee', u'NOUN'),\n",
       "  (u',', u'.'),\n",
       "  (u'which', u'DET'),\n",
       "  (u'had', u'VERB'),\n",
       "  (u'over-all', u'ADJ'),\n",
       "  (u'charge', u'NOUN'),\n",
       "  (u'of', u'ADP'),\n",
       "  (u'the', u'DET'),\n",
       "  (u'election', u'NOUN'),\n",
       "  (u',', u'.'),\n",
       "  (u'``', u'.'),\n",
       "  (u'deserves', u'VERB'),\n",
       "  (u'the', u'DET'),\n",
       "  (u'praise', u'NOUN'),\n",
       "  (u'and', u'CONJ'),\n",
       "  (u'thanks', u'NOUN'),\n",
       "  (u'of', u'ADP'),\n",
       "  (u'the', u'DET'),\n",
       "  (u'City', u'NOUN'),\n",
       "  (u'of', u'ADP'),\n",
       "  (u'Atlanta', u'NOUN'),\n",
       "  (u\"''\", u'.'),\n",
       "  (u'for', u'ADP'),\n",
       "  (u'the', u'DET'),\n",
       "  (u'manner', u'NOUN'),\n",
       "  (u'in', u'ADP'),\n",
       "  (u'which', u'DET'),\n",
       "  (u'the', u'DET'),\n",
       "  (u'election', u'NOUN'),\n",
       "  (u'was', u'VERB'),\n",
       "  (u'conducted', u'VERB'),\n",
       "  (u'.', u'.')]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_news_tagged_sents = brown.tagged_sents(categories='news', tagset='universal')\n",
    "brown_news_tagged_sents[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100554 16413\n"
     ]
    }
   ],
   "source": [
    "tmpwords = brown.tagged_words(categories='news')\n",
    "print len(tmpwords), len(set(tmpwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---- word classification\n",
    "\n",
    "from nltk.corpus import names\n",
    "\n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] + \\\n",
    "                 [(name, 'female') for name in names.words('female.txt')])\n",
    "\n",
    "import random\n",
    "random.shuffle(labeled_names)\n",
    "\n",
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
    "\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# document classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gender_features2(name):\n",
    "    features = {}\n",
    "    features[\"first_letter\"] = name[0].lower()\n",
    "    features[\"last_letter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count({})\".format(letter)] = name.lower().count(letter)\n",
    "        features[\"has({})\".format(letter)] = (letter in name.lower())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# ---- Document classification \n",
    "\n",
    "# ! Movie reviews datatset\n",
    "\n",
    "#  document-term features \n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\\\n",
    "             for category in movie_reviews.categories()\\\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "print len(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'contains(caned)': False, 'contains(canet)': False, 'contains(localized)': False, 'contains(slothful)': False, 'contains(woods)': False, 'contains(francesca)': False, 'contains(originality)': False, 'contains(scold)': False, 'contains(hennings)': False, 'contains(sonja)': False, 'contains(spiders)': False, 'contains(sucess)': False, 'contains(askew)': False, 'contains(stipulate)': False, 'contains(rickman)': False, 'contains(hanging)': False, 'contains(bazooms)': False, 'contains(comically)': False, 'contains(wracked)': False, 'contains(disobeying)': False}\n"
     ]
    }
   ],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:20]\n",
    "\n",
    "def document_features(document):\n",
    "    \n",
    "    document_words = set(document)\n",
    "    \n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "# featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "\n",
    "print document_features(documents[10][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---- part-of-speech tagging\n",
    "# ! Brown dataset: POS tagging\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---- Identifying Dialogue Act Types\n",
    "\n",
    "# ! Treebank dataset\n",
    "\n",
    "sents = nltk.corpus.treebank_raw.sents()\n",
    "print sents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'next-word-capitalized': False, 'prev-word': u'nov', 'prev-word-is-one-char': False, 'punct': u'.'}, False)\n"
     ]
    }
   ],
   "source": [
    "# ---- Sentence Segmentation\n",
    "\n",
    "def punct_features(tokens, i):\n",
    "    return {'next-word-capitalized': tokens[i+1][0].isupper(),\n",
    "            'prev-word': tokens[i-1].lower(),\n",
    "            'punct': tokens[i],\n",
    "            'prev-word-is-one-char': len(tokens[i-1]) == 1}\n",
    "\n",
    "\n",
    "sents = nltk.corpus.treebank_raw.sents()\n",
    "tokens = []\n",
    "boundaries = set()\n",
    "offset = 0\n",
    "for sent in sents:\n",
    "    tokens.extend(sent)\n",
    "    offset += len(sent)\n",
    "    boundaries.add(offset-1)\n",
    "    \n",
    "featuresets = [(punct_features(tokens, i), (i in boundaries)) \\\n",
    "               for i in range(1, len(tokens)-1) if tokens[i] in '.?!']\n",
    "\n",
    "print featuresets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---- Recognizing Textual Entailment\n",
    "\n",
    "def rte_features(rtepair):\n",
    "    extractor = nltk.RTEFeatureExtractor(rtepair)\n",
    "    features = {}\n",
    "    features['word_overlap'] = len(extractor.overlap('word'))\n",
    "    features['word_hyp_extra'] = len(extractor.hyp_extra('word'))\n",
    "    features['ne_overlap'] = len(extractor.overlap('ne'))\n",
    "    features['ne_hyp_extra'] = len(extractor.hyp_extra('ne'))\n",
    "    return features\n",
    "\n",
    "rtepair = nltk.corpus.rte.pairs(['rte3_dev.xml'])[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Alphabetical list of part-of-speech tags used in the Penn Treebank Project:\n",
    "Number\n",
    "Tag\n",
    "Description\n",
    "1.\tCC\tCoordinating conjunction\n",
    "2.\tCD\tCardinal number\n",
    "3.\tDT\tDeterminer\n",
    "4.\tEX\tExistential there\n",
    "5.\tFW\tForeign word\n",
    "6.\tIN\tPreposition or subordinating conjunction\n",
    "7.\tJJ\tAdjective\n",
    "8.\tJJR\tAdjective, comparative\n",
    "9.\tJJS\tAdjective, superlative\n",
    "10.\tLS\tList item marker\n",
    "11.\tMD\tModal\n",
    "12.\tNN\tNoun, singular or mass\n",
    "13.\tNNS\tNoun, plural\n",
    "14.\tNNP\tProper noun, singular\n",
    "15.\tNNPS\tProper noun, plural\n",
    "16.\tPDT\tPredeterminer\n",
    "17.\tPOS\tPossessive ending\n",
    "18.\tPRP\tPersonal pronoun\n",
    "19.\tPRP$\tPossessive pronoun\n",
    "20.\tRB\tAdverb\n",
    "21.\tRBR\tAdverb, comparative\n",
    "22.\tRBS\tAdverb, superlative\n",
    "23.\tRP\tParticle\n",
    "24.\tSYM\tSymbol\n",
    "25.\tTO\tto\n",
    "26.\tUH\tInterjection\n",
    "27.\tVB\tVerb, base form\n",
    "28.\tVBD\tVerb, past tense\n",
    "29.\tVBG\tVerb, gerund or present participle\n",
    "30.\tVBN\tVerb, past participle\n",
    "31.\tVBP\tVerb, non-3rd person singular present\n",
    "32.\tVBZ\tVerb, 3rd person singular present\n",
    "33.\tWDT\tWh-determiner\n",
    "34.\tWP\tWh-pronoun\n",
    "35.\tWP$\tPossessive wh-pronoun\n",
    "36.\tWRB\tWh-adverb\n",
    "\n",
    "IOB chunk tag:  I (inside), O (outside), or B (begin)\n",
    "\n",
    "chunk: NP, VP and PP\n",
    "\n",
    "Symbol\tMeaning\tExample\n",
    "S\tsentence\tthe man walked\n",
    "NP\tnoun phrase\ta dog\n",
    "VP\tverb phrase\tsaw a park\n",
    "PP\tprepositional phrase\twith a telescope\n",
    "Det\tdeterminer\tthe\n",
    "N\tnoun\tdog\n",
    "V\tverb\twalked\n",
    "P\tpreposition\tin\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3897 .START \n",
      "\n",
      "Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29.\n",
      "[(u'.START', 'JJ'), (u'Pierre', 'NNP'), (u'Vinken', 'NNP'), (u',', ','), (u'61', 'CD'), (u'years', 'NNS'), (u'old', 'JJ'), (u',', ','), (u'will', 'MD'), (u'join', 'VB'), (u'the', 'DT'), (u'board', 'NN'), (u'as', 'IN'), (u'a', 'DT'), (u'nonexecutive', 'JJ'), (u'director', 'NN'), (u'Nov.', 'NNP'), (u'29', 'CD'), (u'.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# ---- information extraction\n",
    "\n",
    "# ---- POS tag\n",
    "\n",
    "def ie_preprocess(document):\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    \n",
    "    print len(sentences), sentences[0]\n",
    "    \n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "doc = nltk.corpus.treebank_raw.raw()\n",
    "sent_tag = ie_preprocess(doc)\n",
    "\n",
    "print sent_tag[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n",
      "[('the', 'DT'), ('little', 'JJ'), ('yellow', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('cat', 'NN')]\n",
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "# ---- chunking\n",
    "\n",
    "# method 1: to chunk-tag the POS-tag\n",
    "# method 2: classifier based \n",
    "\n",
    "sentence_pos = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),\\\n",
    "            (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"), \\\n",
    "                (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(sentence_pos)\n",
    "\n",
    "print result\n",
    "\n",
    "# (S\n",
    "#   (NP the/DT little/JJ yellow/JJ dog/NN)\n",
    "#   barked/VBD\n",
    "#   at/IN\n",
    "#   (NP the/DT cat/NN))\n",
    "# result.draw() \n",
    "\n",
    "sentence = \"the little yellow dog barked at the cat\"\n",
    "\n",
    "chunkParser = nltk.RegexpParser(grammar)\n",
    "sentence_pos = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "print sentence_pos\n",
    "\n",
    "tree = chunkParser.parse(sentence_pos)\n",
    "print tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2000\n",
    "\n",
    "\n",
    "def npchunk_features(sentence, i, history):\n",
    "    \n",
    "    word, pos = sentence[i]\n",
    "    \n",
    "    return {\"pos\": pos}\n",
    "\n",
    "def npchunk_features_pos(sentence, i, history):\n",
    "    \n",
    "    word, pos = sentence[i]\n",
    "    if i == 0:\n",
    "        prevword, prevpos = \"<START>\", \"<START>\"\n",
    "        \n",
    "    else:\n",
    "        prevword, prevpos = sentence[i-1]\n",
    "\n",
    "    return {\"pos\": pos, \"prevpos\": prevpos}\n",
    "\n",
    "def npchunk_features_pos_word(sentence, i, history):\n",
    "    \n",
    "    word, pos = sentence[i]\n",
    "    \n",
    "    if i == 0:\n",
    "        prevword, prevpos = \"<START>\", \"<START>\"\n",
    "    else:\n",
    "        prevword, prevpos = sentence[i-1]\n",
    "        \n",
    "    return {\"pos\": pos, \"word\": word, \"prevpos\": prevpos}\n",
    "\n",
    "\n",
    "class UnigramChunker(nltk.ChunkParserI):\n",
    "    \n",
    "    '''\n",
    "    return: tree\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, train_sents):\n",
    "        \n",
    "        train_data = \\\n",
    "        [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                for sent in train_sents]\n",
    "        \n",
    "        # built-in Unigram tagger\n",
    "        self.tagger = nltk.UnigramTagger(train_data)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        \n",
    "        pos_tags = [pos for (word, pos) in sentence]\n",
    "        \n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        \n",
    "        chunktags = \\\n",
    "        [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        \n",
    "        conlltags = \\\n",
    "        [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        \n",
    "        return nltk.chunk.conlltags2tree(conlltags)\n",
    "\n",
    "\n",
    "class ConsecutiveNPChunkTagger(nltk.TaggerI):\n",
    "    \n",
    "    '''\n",
    "    return: IOB tags\n",
    "    '''\n",
    "\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            \n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                \n",
    "                featureset = \\\n",
    "                npchunk_features(untagged_sent, i, history)\n",
    "                \n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "                \n",
    "                \n",
    "        self.classifier = nltk.MaxentClassifier.train(\n",
    "            train_set, algorithm='megam', trace=0)\n",
    "        \n",
    "        print '---- test ---', train_set[0]\n",
    "        \n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = npchunk_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n",
    "\n",
    "class ConsecutiveNPChunker(nltk.ChunkParserI):\n",
    "    \n",
    "    '''\n",
    "    return: tree \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, train_sents):\n",
    "        tagged_sents = [[((w,t),c) for (w,t,c) in\n",
    "                         nltk.chunk.tree2conlltags(sent)]\n",
    "                        for sent in train_sents]\n",
    "        self.tagger = ConsecutiveNPChunkTagger(tagged_sents)\n",
    "\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        tagged_sents = self.tagger.tag(sentence)\n",
    "        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
    "\n",
    "chunker = ConsecutiveNPChunker(train_sents)\n",
    "    \n",
    "# nltk.chunk.tree2conlltags(): tree to IOB tags\n",
    "# nltk.chunk.conlltags2tree(): IOB tags to tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import anago\n",
    "from anago.utils import download\n",
    "\n",
    "url = 'https://storage.googleapis.com/chakki/datasets/public/ner/conll2003_en.zip'\n",
    "weights, params, preprocessor = download(url)\n",
    "model = anago.Sequence.load(weights, params, preprocessor)\n",
    "# model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = anago.Sequence.load(weights, params, preprocessor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['President', 'Obama', 'is', 'speaking', 'at', 'the', 'White', 'House', '.']\n",
      "(S\n",
      "  President/NNP\n",
      "  (PERSON Obama/NNP)\n",
      "  is/VBZ\n",
      "  speaking/VBG\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (FACILITY White/NNP House/NNP)\n",
      "  ./.)\n",
      "[('President', 'NNP', 'O'), ('Obama', 'NNP', 'B-PERSON'), ('is', 'VBZ', 'O'), ('speaking', 'VBG', 'O'), ('at', 'IN', 'O'), ('the', 'DT', 'O'), ('White', 'NNP', 'B-FACILITY'), ('House', 'NNP', 'I-FACILITY'), ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# print(nltk.ne_chunk(text))\n",
    "\n",
    "# sentences = nltk.sent_tokenize(text)\n",
    "tokenized_sent = nltk.word_tokenize(text)\n",
    "print(tokenized_sent)\n",
    "tagged_sent = nltk.pos_tag(tokenized_sent)\n",
    "\n",
    "chunked_sent = nltk.ne_chunk(tagged_sent)\n",
    "\n",
    "print(chunked_sent)\n",
    "\n",
    "print(nltk.chunk.tree2conlltags(chunked_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # ---- Named entity recognition\n",
    "\n",
    "sent = nltk.corpus.treebank.tagged_sents()\n",
    "print(len(sent))\n",
    "print(sent[0])\n",
    "# print(nltk.ne_chunk(sent))\n",
    "\n",
    "subset = sent[:5]\n",
    "\n",
    "for i in subset:\n",
    "    print(nltk.ne_chunk(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---- Entity relation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "# ---- sentiment \n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "senti_clf = SentimentIntensityAnalyzer()\n",
    "for sentence in sentences:\n",
    "...     print(sentence)\n",
    "...     ss = sid.polarity_scores(sentence)\n",
    "...     for k in sorted(ss):\n",
    "...         print('{0}: {1}, '.format(k, ss[k]), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Coreference\n",
    "resolution is applied to further link entity\n",
    "mentions across sentence boundaries providing\n",
    "more link targets between entities\n",
    "\n",
    "Nouns are grouped, if they are directly\n",
    "adjacent and connected through a dependency link\n",
    "of the type poss or nsubj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OntoNotes 5.0 dataset (the largest coreference annotated corpus)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
